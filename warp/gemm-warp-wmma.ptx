//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_75
.address_size 64

	// .globl	_Z4GEMMP6__halfS0_PfS1_iii
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem has been demoted
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem has been demoted
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem has been demoted

.visible .entry _Z4GEMMP6__halfS0_PfS1_iii(
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_0,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_1,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_2,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_3,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_4,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_5,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_6
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<50>;
	.reg .b32 	%r<159>;
	.reg .b64 	%rd<21>;
	// demoted variable
	.shared .align 2 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem[4096];
	// demoted variable
	.shared .align 2 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem[4096];
	// demoted variable
	.shared .align 4 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem[16384];

	ld.param.u64 	%rd4, [_Z4GEMMP6__halfS0_PfS1_iii_param_0];
	ld.param.u64 	%rd5, [_Z4GEMMP6__halfS0_PfS1_iii_param_1];
	ld.param.u64 	%rd6, [_Z4GEMMP6__halfS0_PfS1_iii_param_2];
	ld.param.u32 	%r45, [_Z4GEMMP6__halfS0_PfS1_iii_param_5];
	ld.param.u32 	%r46, [_Z4GEMMP6__halfS0_PfS1_iii_param_6];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r2, %ntid.x;
	mul.lo.s32 	%r3, %r1, %r2;
	mov.u32 	%r47, %ntid.z;
	mul.lo.s32 	%r4, %r3, %r47;
	setp.lt.s32	%p2, %r4, 96;
	@%p2 bra 	BB0_2;
	bra.uni 	BB0_1;

BB0_2:
	shr.s32 	%r54, %r4, 31;
	shr.u32 	%r55, %r54, 27;
	add.s32 	%r56, %r4, %r55;
	shr.s32 	%r149, %r56, 5; //linear_warpid
	mov.u32 	%r150, 32; //warpsizse
	bra.uni 	BB0_3;

BB0_1:
	shr.s32 	%r49, %r4, 31;
	shr.u32 	%r50, %r49, 26;
	add.s32 	%r51, %r4, %r50;
	shr.s32 	%r52, %r51, 6;
	shl.b32 	%r150, %r52, 5;
	mov.u32 	%r149, 2;

BB0_3:
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r57, %tid.z;
	mov.u32 	%r58, %tid.y;
	mad.lo.s32 	%r59, %r57, %r1, %r58;
	mov.u32 	%r60, %tid.x;
	mad.lo.s32 	%r9, %r59, %r2, %r60;// linear_tid
	shr.s32 	%r61, %r9, 31;
	shr.u32 	%r62, %r61, 27;
	add.s32 	%r63, %r9, %r62;
	shr.s32 	%r64, %r63, 5;
	rem.s32 	%r10, %r64, %r149; //warpIdx.x
	div.s32 	%r11, %r64, %r149; // warp_Idx.y
	mov.u32 	%r65, %ctaid.y;
	shl.b32 	%r12, %r65, 6;//i_iter_tile_base
	mov.u32 	%r66, %ctaid.x;
	shl.b32 	%r13, %r66, 6; //j_iter_tile_base
	mad.lo.s32 	%r14, %r12, %r45, %r13;
	setp.gt.s32	%p3, %r9, 4095;
	@%p3 bra 	BB0_6;

	mov.u32 	%r151, %r9;// linear_tid

BB0_5: // c matrix copy loop into shared memory
	shr.s32 	%r67, %r151, 31;
	shr.u32 	%r68, %r67, 26;
	add.s32 	%r69, %r151, %r68;
	shr.s32 	%r70, %r69, 6;
	and.b32  	%r71, %r69, -64;
	sub.s32 	%r72, %r151, %r71;
	mad.lo.s32 	%r73, %r70, %r45, %r72;
	add.s32 	%r74, %r14, %r73;
	mul.wide.s32 	%rd7, %r74, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.f32 	%f9, [%rd8];
	shl.b32 	%r75, %r151, 2;
	mov.u32 	%r76, _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem;
	add.s32 	%r77, %r76, %r75;
	st.shared.f32 	[%r77], %f9;
	add.s32 	%r151, %r151, %r3;
	setp.lt.s32	%p4, %r151, 4096;
	@%p4 bra 	BB0_5;

BB0_6:
	bar.sync 	0;
	setp.lt.s32	%p5, %r46, 1;
	@%p5 bra 	BB0_20;

	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
	mul.lo.s32 	%r17, %r12, %r46;
	shl.b32 	%r18, %r11, 5;
	shl.b32 	%r19, %r10, 5;
	shl.b32 	%r20, %r149, 5;
	shl.b32 	%r79, %r10, 6;
	mov.u32 	%r80, _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem;
	add.s32 	%r21, %r80, %r79;
	shl.b32 	%r22, %r149, 6;
	shl.b32 	%r81, %r10, 7;
	mov.u32 	%r82, _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem;
	add.s32 	%r23, %r82, %r81;
	shl.b32 	%r24, %r149, 7;
	mov.u32 	%r152, 0;

BB0_8:
	add.s32 	%r26, %r152, %r17;
	mad.lo.s32 	%r27, %r152, %r45, %r13;
	setp.gt.s32	%p6, %r9, 2047;
	mov.u32 	%r153, %r9;
	@%p6 bra 	BB0_12;

BB0_9:
	shr.s32 	%r83, %r153, 31;
	shr.u32 	%r84, %r83, 27;
	add.s32 	%r85, %r153, %r84;
	shr.s32 	%r86, %r85, 5;
	and.b32  	%r87, %r85, -32;
	sub.s32 	%r88, %r153, %r87;
	shl.b32 	%r89, %r153, 1;
	mov.u32 	%r90, _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem;
	add.s32 	%r91, %r90, %r89;
	mad.lo.s32 	%r92, %r86, %r46, %r88;
	add.s32 	%r93, %r26, %r92;
	mul.wide.s32 	%rd9, %r93, 2;
	add.s64 	%rd10, %rd2, %rd9;
	ld.global.u16 	%rs1, [%rd10];
	st.shared.u16 	[%r91], %rs1;
	add.s32 	%r153, %r153, %r3;
	setp.lt.s32	%p7, %r153, 2048;
	@%p7 bra 	BB0_9;

	mov.u32 	%r154, %r9;
	@%p6 bra 	BB0_12;

BB0_11:
	shl.b32 	%r94, %r154, 1;
	add.s32 	%r96, %r80, %r94;
	shr.s32 	%r97, %r154, 31;
	shr.u32 	%r98, %r97, 26;
	add.s32 	%r99, %r154, %r98;
	shr.s32 	%r100, %r99, 6;
	and.b32  	%r101, %r99, -64;
	sub.s32 	%r102, %r154, %r101;
	mad.lo.s32 	%r103, %r100, %r45, %r102;
	add.s32 	%r104, %r27, %r103;
	mul.wide.s32 	%rd11, %r104, 2;
	add.s64 	%rd12, %rd3, %rd11;
	ld.global.u16 	%rs2, [%rd12];
	st.shared.u16 	[%r96], %rs2;
	add.s32 	%r154, %r154, %r3;
	setp.lt.s32	%p9, %r154, 2048;
	@%p9 bra 	BB0_11;

BB0_12:
	setp.lt.s32	%p1, %r18, 64;
	bar.sync 	0;
	mov.u32 	%r155, %r18;
	@!%p1 bra 	BB0_19;
	bra.uni 	BB0_13;

BB0_13:
	setp.gt.s32	%p10, %r19, 63;
	@%p10 bra 	BB0_18;

	shl.b32 	%r105, %r155, 6;
	mov.u32 	%r106, _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem;
	add.s32 	%r34, %r106, %r105;
	add.s32 	%r35, %r34, 32;
	mul.lo.s32 	%r107, %r45, %r155;
	shl.b32 	%r108, %r107, 2;
	add.s32 	%r156, %r23, %r108;
	mov.u32 	%r157, %r21;
	mov.u32 	%r158, %r19;

BB0_15:
	mov.u32 	%r109, 64;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f10, %f11, %f12, %f13, %f14, %f15, %f16, %f17}, [%r156], %r109;
	add.s32 	%r110, %r156, 64;			// offset in number of bytes.
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f18, %f19, %f20, %f21, %f22, %f23, %f24, %f25}, [%r110], %r109;
	add.s32 	%r111, %r156, 4096;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f26, %f27, %f28, %f29, %f30, %f31, %f32, %f33}, [%r111], %r109;
	add.s32 	%r112, %r156, 4160;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f34, %f35, %f36, %f37, %f38, %f39, %f40, %f41}, [%r112], %r109;
	mov.u32 	%r113, 32;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r114, %r115, %r116, %r117, %r118, %r119, %r120, %r121}, [%r34], %r113;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r122, %r123, %r124, %r125, %r126, %r127, %r128, %r129}, [%r157], %r109;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8}, {%r114, %r115, %r116, %r117, %r118, %r119, %r120, %r121}, {%r122, %r123, %r124, %r125, %r126, %r127, %r128, %r129}, {%f10, %f11, %f12, %f13, %f14, %f15, %f16, %f17};
	setp.lt.s32	%p11, %r46, 17;
	@%p11 bra 	BB0_17;

	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r131, %r132, %r133, %r134, %r135, %r136, %r137, %r138}, [%r35], %r113;
	add.s32 	%r139, %r157, 2048;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r141, %r142, %r143, %r144, %r145, %r146, %r147, %r148}, [%r139], %r109;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f42, %f43, %f44, %f45, %f46, %f47, %f48, %f49}, {%r131, %r132, %r133, %r134, %r135, %r136, %r137, %r138}, {%r141, %r142, %r143, %r144, %r145, %r146, %r147, %r148}, {%f1, %f2, %f3, %f4, %f5, %f6, %f7, %f8};

BB0_17:
	add.s32 	%r157, %r157, %r22;
	add.s32 	%r156, %r156, %r24;
	add.s32 	%r158, %r158, %r20;
	setp.lt.s32	%p12, %r158, 64;
	@%p12 bra 	BB0_15;

BB0_18:
	add.s32 	%r155, %r155, %r150;
	setp.lt.s32	%p13, %r155, 64;
	@%p13 bra 	BB0_13;

BB0_19:
	add.s32 	%r152, %r152, 32;
	setp.lt.s32	%p14, %r152, %r46;
	@%p14 bra 	BB0_8;

BB0_20:
	ret;
}


