//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-27506705
// Cuda compilation tools, release 10.2, V10.2.89
// Based on LLVM 3.4svn
//

.version 6.5
.target sm_75
.address_size 64

	// .globl	_Z4GEMMP6__halfS0_PfS1_iii
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem has been demoted
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem has been demoted
// _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem has been demoted

.visible .entry _Z4GEMMP6__halfS0_PfS1_iii(
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_0,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_1,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_2,
	.param .u64 _Z4GEMMP6__halfS0_PfS1_iii_param_3,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_4,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_5,
	.param .u32 _Z4GEMMP6__halfS0_PfS1_iii_param_6
)
{
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<516>;
	.reg .b32 	%r<155>;
	.reg .b64 	%rd<86>;
	// demoted variable
	.shared .align 2 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem[2048];
	// demoted variable
	.shared .align 2 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem[2048];
	// demoted variable
	.shared .align 4 .b8 _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem[16384];

	ld.param.u64 	%rd16, [_Z4GEMMP6__halfS0_PfS1_iii_param_0];
	ld.param.u64 	%rd17, [_Z4GEMMP6__halfS0_PfS1_iii_param_1];
	ld.param.u64 	%rd18, [_Z4GEMMP6__halfS0_PfS1_iii_param_2];
	ld.param.u64 	%rd19, [_Z4GEMMP6__halfS0_PfS1_iii_param_3];
	ld.param.u32 	%r32, [_Z4GEMMP6__halfS0_PfS1_iii_param_5];
	ld.param.u32 	%r33, [_Z4GEMMP6__halfS0_PfS1_iii_param_6];
	mov.u32 	%r1, %ntid.y;
	mov.u32 	%r2, %ntid.x;
	mul.lo.s32 	%r3, %r1, %r2;
	mov.u32 	%r34, %ntid.z;
	mul.lo.s32 	%r4, %r3, %r34;
	setp.lt.s32	%p2, %r4, 96;
	@%p2 bra 	BB0_2;
	bra.uni 	BB0_1;

BB0_2:
	shr.s32 	%r41, %r4, 31;
	shr.u32 	%r42, %r41, 27;
	add.s32 	%r43, %r4, %r42;
	shr.s32 	%r147, %r43, 5;
	mov.u32 	%r148, 32;
	bra.uni 	BB0_3;

BB0_1:
	shr.s32 	%r36, %r4, 31;
	shr.u32 	%r37, %r36, 26;
	add.s32 	%r38, %r4, %r37;
	shr.s32 	%r39, %r38, 6;
	shl.b32 	%r148, %r39, 5;
	mov.u32 	%r147, 2;

BB0_3:
	mov.u32 	%r44, %tid.z;
	mov.u32 	%r45, %tid.y;
	mad.lo.s32 	%r46, %r44, %r1, %r45;
	mov.u32 	%r47, %tid.x;
	mad.lo.s32 	%r9, %r46, %r2, %r47;
	shr.s32 	%r48, %r9, 31;
	shr.u32 	%r49, %r48, 27;
	add.s32 	%r50, %r9, %r49;
	shr.s32 	%r51, %r50, 5;
	rem.s32 	%r10, %r51, %r147;
	div.s32 	%r11, %r51, %r147;
	mov.u32 	%r52, %ctaid.y;
	shl.b32 	%r12, %r52, 6;
	mov.u32 	%r53, %ctaid.x;
	shl.b32 	%r54, %r53, 6;
	mul.lo.s32 	%r55, %r12, %r32;
	cvt.s64.s32	%rd20, %r55;
	cvt.s64.s32	%rd1, %r54;
	add.s64 	%rd2, %rd20, %rd1;
	cvta.to.global.u64 	%rd3, %rd18;
	setp.gt.s32	%p3, %r9, 4095;
	@%p3 bra 	BB0_6;

	mov.u32 	%r149, %r9;

BB0_5:
	shr.s32 	%r56, %r149, 31;
	shr.u32 	%r57, %r56, 26;
	add.s32 	%r58, %r149, %r57;
	shr.s32 	%r59, %r58, 6;
	and.b32  	%r60, %r58, -64;
	sub.s32 	%r61, %r149, %r60;
	mul.lo.s32 	%r62, %r59, %r32;
	cvt.s64.s32	%rd21, %r62;
	add.s64 	%rd22, %rd21, %rd2;
	cvt.s64.s32	%rd23, %r61;
	add.s64 	%rd24, %rd22, %rd23;
	shl.b64 	%rd25, %rd24, 2;
	add.s64 	%rd26, %rd3, %rd25;
	ld.global.f32 	%f289, [%rd26];
	shl.b32 	%r63, %r149, 2;
	mov.u32 	%r64, _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem;
	add.s32 	%r65, %r64, %r63;
	st.shared.f32 	[%r65], %f289;
	add.s32 	%r149, %r149, %r3;
	setp.lt.s32	%p4, %r149, 4096;
	@%p4 bra 	BB0_5;

BB0_6:
	bar.sync 	0;
	setp.lt.s32	%p5, %r33, 1;
	@%p5 bra 	BB0_22;

	cvta.to.global.u64 	%rd4, %rd16;
	cvta.to.global.u64 	%rd5, %rd17;
	mul.lo.s32 	%r67, %r12, %r33;
	cvt.s64.s32	%rd6, %r67;
	shl.b32 	%r15, %r9, 2;
	shl.b32 	%r16, %r3, 2;
	shl.b32 	%r17, %r11, 5;
	shl.b32 	%r18, %r10, 5;
	shl.b32 	%r19, %r147, 5;
	cvta.to.global.u64 	%rd7, %rd19;
	mov.u32 	%r150, 0;

BB0_8:
	cvt.s64.s32	%rd29, %r150;
	add.s64 	%rd9, %rd29, %rd6;
	mul.lo.s32 	%r68, %r150, %r32;
	cvt.s64.s32	%rd30, %r68;
	add.s64 	%rd10, %rd30, %rd1;
	setp.gt.s32	%p6, %r15, 1023;
	mov.u32 	%r151, %r15;
	@%p6 bra 	BB0_12;

BB0_9:
	shr.s32 	%r69, %r151, 31;
	shr.u32 	%r70, %r69, 28;
	add.s32 	%r71, %r151, %r70;
	shr.s32 	%r72, %r71, 4;
	and.b32  	%r73, %r71, -16;
	sub.s32 	%r74, %r151, %r73;
	mad.lo.s32 	%r75, %r72, %r33, %r74;
	shl.b32 	%r76, %r151, 1;
	mov.u32 	%r77, _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem;
	add.s32 	%r78, %r77, %r76;
	cvt.u64.u32	%rd31, %r75;
	add.s64 	%rd32, %rd9, %rd31;
	shl.b64 	%rd33, %rd32, 1;
	add.s64 	%rd34, %rd4, %rd33;
	ld.global.u16 	%rs1, [%rd34];
	st.shared.u16 	[%r78], %rs1;
	add.s32 	%r79, %r75, 1;
	cvt.u64.u32	%rd35, %r79;
	add.s64 	%rd36, %rd9, %rd35;
	shl.b64 	%rd37, %rd36, 1;
	add.s64 	%rd38, %rd4, %rd37;
	ld.global.u16 	%rs2, [%rd38];
	st.shared.u16 	[%r78+2], %rs2;
	add.s32 	%r80, %r75, 2;
	cvt.u64.u32	%rd39, %r80;
	add.s64 	%rd40, %rd9, %rd39;
	shl.b64 	%rd41, %rd40, 1;
	add.s64 	%rd42, %rd4, %rd41;
	ld.global.u16 	%rs3, [%rd42];
	st.shared.u16 	[%r78+4], %rs3;
	add.s32 	%r81, %r75, 3;
	cvt.u64.u32	%rd43, %r81;
	add.s64 	%rd44, %rd9, %rd43;
	shl.b64 	%rd45, %rd44, 1;
	add.s64 	%rd46, %rd4, %rd45;
	ld.global.u16 	%rs4, [%rd46];
	st.shared.u16 	[%r78+6], %rs4;
	add.s32 	%r151, %r151, %r16;
	setp.lt.s32	%p7, %r151, 1024;
	@%p7 bra 	BB0_9;

	mov.u32 	%r152, %r15;
	@%p6 bra 	BB0_12;

BB0_11:
	shr.s32 	%r82, %r152, 31;
	shr.u32 	%r83, %r82, 26;
	add.s32 	%r84, %r152, %r83;
	shr.s32 	%r85, %r84, 6;
	and.b32  	%r86, %r84, -64;
	sub.s32 	%r87, %r152, %r86;
	mad.lo.s32 	%r88, %r85, %r32, %r87;
	shl.b32 	%r89, %r152, 1;
	mov.u32 	%r90, _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem;
	add.s32 	%r91, %r90, %r89;
	cvt.u64.u32	%rd47, %r88;
	add.s64 	%rd48, %rd10, %rd47;
	shl.b64 	%rd49, %rd48, 1;
	add.s64 	%rd50, %rd5, %rd49;
	ld.global.u16 	%rs5, [%rd50];
	st.shared.u16 	[%r91], %rs5;
	add.s32 	%r92, %r88, 1;
	cvt.u64.u32	%rd51, %r92;
	add.s64 	%rd52, %rd10, %rd51;
	shl.b64 	%rd53, %rd52, 1;
	add.s64 	%rd54, %rd5, %rd53;
	ld.global.u16 	%rs6, [%rd54];
	st.shared.u16 	[%r91+2], %rs6;
	add.s32 	%r93, %r88, 2;
	cvt.u64.u32	%rd55, %r93;
	add.s64 	%rd56, %rd10, %rd55;
	shl.b64 	%rd57, %rd56, 1;
	add.s64 	%rd58, %rd5, %rd57;
	ld.global.u16 	%rs7, [%rd58];
	st.shared.u16 	[%r91+4], %rs7;
	add.s32 	%r94, %r88, 3;
	cvt.u64.u32	%rd59, %r94;
	add.s64 	%rd60, %rd10, %rd59;
	shl.b64 	%rd61, %rd60, 1;
	add.s64 	%rd62, %rd5, %rd61;
	ld.global.u16 	%rs8, [%rd62];
	st.shared.u16 	[%r91+6], %rs8;
	add.s32 	%r152, %r152, %r16;
	setp.lt.s32	%p9, %r152, 1024;
	@%p9 bra 	BB0_11;

BB0_12:
	setp.lt.s32	%p1, %r17, 64;
	bar.sync 	0;
	@!%p1 bra 	BB0_21;
	bra.uni 	BB0_13;

BB0_13:
	mov.u32 	%r153, %r17;

BB0_14:
	setp.gt.s32	%p10, %r18, 63;
	@%p10 bra 	BB0_20;

	shl.b32 	%r26, %r153, 6;
	shl.b32 	%r95, %r153, 5;
	mov.u32 	%r96, _ZZ4GEMMP6__halfS0_PfS1_iiiE5asmem;
	add.s32 	%r27, %r96, %r95;
	mov.u32 	%r154, %r18;

BB0_16:
	mov.u32 	%r28, %r154;
	setp.ne.s32	%p11, %r150, 0;
	@%p11 bra 	BB0_18;

	mov.u32 	%r97, _ZZ4GEMMP6__halfS0_PfS1_iiiE5csmem;
	add.s32 	%r98, %r28, %r26;
	shl.b32 	%r99, %r98, 2;
	add.s32 	%r100, %r99, %r97;
	mov.u32 	%r101, 64;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f324, %f325, %f326, %f327, %f328, %f329, %f330, %f331}, [%r100], %r101;
	add.s32 	%r102, %r100, 64;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f332, %f333, %f334, %f335, %f336, %f337, %f338, %f339}, [%r102], %r101;
	add.s32 	%r103, %r100, 4096;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f340, %f341, %f342, %f343, %f344, %f345, %f346, %f347}, [%r103], %r101;
	add.s32 	%r104, %r100, 4160;
	wmma.load.c.sync.aligned.row.m16n16k16.shared.f32 	{%f348, %f349, %f350, %f351, %f352, %f353, %f354, %f355}, [%r104], %r101;

BB0_18:
	add.s32 	%r105, %r27, 512;
	mov.u32 	%r106, 16;
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r107, %r108, %r109, %r110, %r111, %r112, %r113, %r114}, [%r27], %r106;
	shl.b32 	%r115, %r28, 1;
	mov.u32 	%r116, _ZZ4GEMMP6__halfS0_PfS1_iiiE5bsmem;
	mov.u32 	%r117, 64;
	add.s32 	%r118, %r116, %r115;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r119, %r120, %r121, %r122, %r123, %r124, %r125, %r126}, [%r118], %r117;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f324, %f325, %f326, %f327, %f328, %f329, %f330, %f331}, {%r107, %r108, %r109, %r110, %r111, %r112, %r113, %r114}, {%r119, %r120, %r121, %r122, %r123, %r124, %r125, %r126}, {%f324, %f325, %f326, %f327, %f328, %f329, %f330, %f331};
	add.s32 	%r127, %r118, 32;
	wmma.load.b.sync.aligned.row.m16n16k16.shared.f16 	{%r128, %r129, %r130, %r131, %r132, %r133, %r134, %r135}, [%r127], %r117;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f332, %f333, %f334, %f335, %f336, %f337, %f338, %f339}, {%r107, %r108, %r109, %r110, %r111, %r112, %r113, %r114}, {%r128, %r129, %r130, %r131, %r132, %r133, %r134, %r135}, {%f332, %f333, %f334, %f335, %f336, %f337, %f338, %f339};
	wmma.load.a.sync.aligned.row.m16n16k16.shared.f16 	{%r136, %r137, %r138, %r139, %r140, %r141, %r142, %r143}, [%r105], %r106;
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f340, %f341, %f342, %f343, %f344, %f345, %f346, %f347}, {%r136, %r137, %r138, %r139, %r140, %r141, %r142, %r143}, {%r119, %r120, %r121, %r122, %r123, %r124, %r125, %r126}, {%f340, %f341, %f342, %f343, %f344, %f345, %f346, %f347};
	wmma.mma.sync.aligned.row.row.m16n16k16.f32.f32 {%f348, %f349, %f350, %f351, %f352, %f353, %f354, %f355}, {%r136, %r137, %r138, %r139, %r140, %r141, %r142, %r143}, {%r128, %r129, %r130, %r131, %r132, %r133, %r134, %r135}, {%f348, %f349, %f350, %f351, %f352, %f353, %f354, %f355};
	add.s32 	%r154, %r28, %r19;
	setp.lt.s32	%p12, %r154, 64;
	@%p12 bra 	BB0_16;

	mul.lo.s32 	%r144, %r153, %r32;
	cvt.s64.s32	%rd71, %r144;
	add.s64 	%rd72, %rd71, %rd2;
	cvt.s64.s32	%rd73, %r28;
	add.s64 	%rd74, %rd72, %rd73;
	shl.b64 	%rd75, %rd74, 2;
	add.s64 	%rd82, %rd7, %rd75;

BB0_20:
	add.s32 	%r153, %r153, %r148;
	setp.lt.s32	%p13, %r153, 64;
	@%p13 bra 	BB0_14;

BB0_21:
	add.s32 	%r150, %r150, 16;
	setp.lt.s32	%p14, %r150, %r33;
	@%p14 bra 	BB0_8;

BB0_22:
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd82], {%f324, %f325, %f326, %f327, %f328, %f329, %f330, %f331}, %r32;
	add.s64 	%rd76, %rd82, 64;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd76], {%f332, %f333, %f334, %f335, %f336, %f337, %f338, %f339}, %r32;
	shl.b32 	%r145, %r32, 4;
	mul.wide.s32 	%rd77, %r145, 4;
	add.s64 	%rd78, %rd82, %rd77;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd78], {%f340, %f341, %f342, %f343, %f344, %f345, %f346, %f347}, %r32;
	add.s32 	%r146, %r145, 16;
	mul.wide.s32 	%rd79, %r146, 4;
	add.s64 	%rd80, %rd82, %rd79;
	wmma.store.d.sync.aligned.row.m16n16k16.global.f32 	[%rd80], {%f348, %f349, %f350, %f351, %f352, %f353, %f354, %f355}, %r32;
	ret;
}


